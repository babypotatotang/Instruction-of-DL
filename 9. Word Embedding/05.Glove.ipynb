{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1oRAiD96dc1-Hww7vg8IoDcXnjjOjW7sp",
      "authorship_tag": "ABX9TyO0UmA7Eh/6M8wmPWunZf7v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babypotatotang/Introduction-to-DeepLearning/blob/main/9.%20Word%20Embedding/05.Glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **GloVe**(Global Vectors for Word Representation)은 카운트 기반과 예측 기반을 모두 사용하는 방법론으로, 기존 카운트 기반의 LSA와 예측 기반의 Word2Vec의 단점을 보완하여 개발되었음. \n",
        "\n",
        "# **1. 기존 방법론에 대한 비판**\n",
        "--- \n",
        "* LSA는 각 단어의 빈도수를 카운트한 행렬이라는 전체적인 통계 정보를 입력으로 받아 차원을 축소하여 잠재된 의미를 끌어내는 방법론\n",
        "* Word2Vec은 실제값과 예측값에 대한 오차를 손실함수를 통해 줄여나가며 학습하는 예측 기반 방법론임. \n",
        "\n",
        "### 각 방식의 장, 단점\n",
        "* LSA는 카운트 기반으로 전체적인 통계 정보를 고려하지만 유추작업에서는 성능이 떨어지고, Word2Vec은 예측 기반으로 유추 작업에서 성능은 뛰어나지만 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체 정보를 반영하지 못함. \n",
        "\n",
        "# **2. Window based Co-occurrence Matrix)**\n",
        "--- \n",
        "* 단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 윈도우 크기 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬을 말함. \n",
        "\n",
        "### ex)  윈도우 크기가 1일때, 아래 텍스트를 가지고 구성한 동시 등장 행렬은 다음과 같음. \n",
        "\n",
        "I like deep learning\n",
        "I like NLP\n",
        "I enjoy flying \n",
        "\n",
        "|카운트|I|like|enjoy|deep|learning|NLP|flying\n",
        "|------|---|---|---|---|---|---|---|\n",
        "|I|0|2|1|0|0|0|0|\n",
        "|like|2|0|0|1|0|1|0|\n",
        "|enjoy|1|0|0|0|0|0|1|\n",
        "|deep|0|1|0|0|1|0|0|\n",
        "|learning|0|0|0|1|0|0|0|\n",
        "|NLP|0|1|0|0|0|0|0|\n",
        "|flying|0|0|1|0|0|0|0|\n",
        "\n",
        "* 위 행렬은 행렬을 전치해도 동일한 행렬이 된다는 특징이 있음. \n",
        "\n",
        "# **3. Co-Occurrence Probability**\n",
        "---\n",
        "* 동시 등장 확률 $P(k|i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 득정 단어 i가 등장했을때, 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률\n",
        "* $P(k|i)$에서 i를 중심단어(Center word), k를 주변단어(Context Word)라고 했을때, \n",
        "    * 위에서 배운 동시 등장 행렬에서 중심 단어 i의 행의 모든 값을 더한 값을 분모\n",
        "    * i행 k열의 값을 분자  \n",
        "\n",
        "|동시 등장 확률과 크기 관계 비(ratio)|k=solid|k=gas|k=water|k=fasion\n",
        "|------------------------------------|-------|-----|-------|---------|\n",
        "|P(k l ice)|0.00019|0.000066|0.003|0.000017|\n",
        "|P(k l steam)| 0.000022 | 0.00078 | 0.0022 | 0.000018 |\n",
        "|P(k l ice) / P(k l steam) | 8.9 | 0.085 | 1.36 | 0.96 |\n",
        "\n",
        "* `ice`가 등장했을때 `solid`가 등장할 확률은 `steam`이 등장했을때 `solid`가 나타날 확률보다 높음."
      ],
      "metadata": {
        "id": "sJP-y5M0-Iyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. 손실함수(Loss Function)**\n",
        "### 용어 정리 \n",
        "\n",
        "* $X$ : 동시 등장 행렬(Co-occurrence Matrix)\n",
        "* $X_{ij}$ : 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 j가 등장하는 횟수\n",
        "* $X_i$: 동시 등장 행렬에서 i행의 값을 모두 더한 값\n",
        "* $P_{ik}$: 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k가 등장할 확률\n",
        "    - Ex) P(solid l ice) = 단어 ice가 등장했을 때 단어 solid가 등장할 확률\n",
        "* $\\omega_i$: 중심 단어 i의 임베딩 벡터\n",
        "* $ \\widetilde{\\omega_k}$: 주변 단어 k의 임베딩 벡터\n",
        "\n",
        "* GloVe의 아이디어를 한 줄로 요약하면 **'임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것'** \n",
        "\n",
        "$$F(\\omega_i, ω_j, \\widetilde{\\omega_k}) =  \\frac{P_{ik}}{P_{jk}}$$\n",
        "* 세 개의 벡터에 어떤 함수 $F$를 수행하면 다음의 확률이 나온다는 초기식으로 부터 전개 시작\n",
        "* 함수 $F$는 두 단어 사이의 동시 등장 확률의 크기 관계 비(ratio) 정보를 벡터 공간에 인코딩하는 것이 목적이며, 이를 통해 두 벡터의 차이을 $F$의 입력으로 사용하는 것을 제안함. \n",
        "$$F((\\omega_i-ω_j)^T, \\widetilde{\\omega_k}) =  \\frac{P_{ik}}{P_{jk}}$$\n",
        "* 하지만 우변은 스칼라값이고 좌변은 벡터 값이기 때문에 선형 공간에서 단어 의미 관계를 표현하고자 내적 연산을 사용함.\n",
        "\n",
        "* 이때 주의해야할 점은 중심 단어 $\\omega$와 $\\widetilde{\\omega}$는 무작위로 선택되는 것이므로, 이 둘의 관계를 자유롭게 교환할 수 있게 해야함. 이를 만족하기 위해 함수 $F$는 다음의 식을 만족해야함. \n",
        "$$F(a+b) = F(a)F(b), \\forall a, b \\in ℜ$$\n",
        "\n",
        "* 이때 $a$와 $b$를 각각 기존의 벡터에 적용시킨다면 덧셈이 뺄셈으로 곱셈이 나눗셈으로 바뀌게 됨. \n",
        "$$F(v_1^Tv_2-v_3^Tv_4) = \\frac{v_1^Tv_2}{v_3^Tv_4}, ∀v_1,v_2,v_3,v_4 \\in V$$\n",
        "$$F((\\omega_i - \\omega_j)^T \\widetilde{\\omega_k}) = \\frac{\\omega_i^T\\widetilde{\\omega_k}}{\\omega_j^T\\widetilde{\\omega_k}}$$\n",
        "* 여기서 원래 $F$ 함수의 결과 값은 $$\\frac{P_{ik}}{P_{jk}}$$이었음. \n",
        "$$\\frac{P_{ik}}{P_{jk}} =  \\frac{\\omega_i^T\\widetilde{\\omega_k}}{\\omega_j^T\\widetilde{\\omega_k}}$$\n",
        "이 식을 풀어쓰면 \n",
        "\n",
        "$$ F(\\omega_i^T\\widetilde{\\omega_k} - \\omega_j^T\\widetilde{\\omega_k}) = \\frac{\\omega_i^T\\widetilde{\\omega_k}}{\\omega_j^T\\widetilde{\\omega_k}} $$\n",
        "\n",
        "* 이를 만족시키는 함수를 지수함수임. \n",
        "\n",
        "### 손실함수\n",
        "$$ Loss Function =  \\sum_{m,n = 1}^V (\\omega_m^T  \\widetilde{\\omega_n} + b_m + \\widetilde{b_n} - logX_{mn})^2 $$\n",
        "* 손실함수는 우변의 값과의 차이를 최소화 하는 방향으로 학습하게 됨. 하지만 다음의 문제로 새로운 가중치 함수를 정의 함. \n",
        "\n",
        "* 문제 1) $logX_{ik}$가 0이 될 수 있음. \n",
        "\n",
        "\n",
        "    -> $log(1+X_{ik})$로 변경 \n",
        "\n",
        "* 문제 2) DTM 처럼 희소 행렬일 가능성이 다분함. 즉 많은 값이 0이거나 너무 많은 값을 가질 수 있음. \n",
        "\n",
        "\n",
        "    ->  이에 대한 가중치를 부여하기로함. 가중치 그래프는 다음과 같음. \n",
        "![](https://wikidocs.net/images/page/22885/%EA%B0%80%EC%A4%91%EC%B9%98.PNG)\n",
        "\n",
        "* $X_{ik}$의 값이 작으면 상대적으로 함수의 값은 작도록 하고, 값이 크면 함수의 값은 상대적으로 크게 설정 이 함수는 다음과 같이 정의 됨. \n",
        "$$f(x) = min(1, (x/x_{max})^0.75)$$\n",
        "\n",
        "* 최종적으로 다음과 같은 일반화 손실함수를 얻을 수 있음. \n",
        "$$ Loss Function =  \\sum_{m,n = 1}^V f(X_{mn}) (\\omega_m^T  \\widetilde{\\omega_n} + b_m + \\widetilde{b_n} - logX_{mn})^2 $$"
      ],
      "metadata": {
        "id": "E6ib-A70N5Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. GloVe 훈련하기**\n",
        "---"
      ],
      "metadata": {
        "id": "ZGtjG7GqKroG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install glove-python-binary"
      ],
      "metadata": {
        "id": "2rO7aYFpKo6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "BjolYMLkMOB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n",
        "\n",
        "# 데이터 전처리 \n",
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)\n",
        "\n",
        "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
        "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "metadata": {
        "id": "xFMQio8fMJnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus()\n",
        "corpus.fit(result, window = 5) # 훈련 데이터로부터  Glove에 사용할 동시 등장 행렬 생성"
      ],
      "metadata": {
        "id": "QBG1sh6XK0vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "metadata": {
        "id": "o-UY8mY32sS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_result1 = glove.most_similar(\"man\")\n",
        "print(model_result1)"
      ],
      "metadata": {
        "id": "vveHDHk35rdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_result2 = glove.most_similar('boy')\n",
        "print(model_result2)"
      ],
      "metadata": {
        "id": "_432RZTV6zBc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}