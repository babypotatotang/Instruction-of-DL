{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNT2U+vvsftDROowz7YQ9K3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babypotatotang/Introduction-to-DeepLearning/blob/main/8.%20NLP%20Preprocess/02.Torchtext%20Tutorial_Eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 파이토치에서는 텍스트에 대한 여러 추상화 기능을 제공하는 자연어 처리 라이브러리 `torchtext`를 제공함. \n",
        "* 제공하는 기능은 다음과 같음. \n",
        "    * file load) 다양한 포맷의 코퍼스 로드\n",
        "    * tokenization) 문장을 단어 단위로 분리\n",
        "    * vocab) 전체 코퍼스의 단어들을 각각의 고유한 정수로 맵핑 \n",
        "    * word vector) 단어 집합의 단어들에 고유한 임베딩 벡터를 만들어 줌. \n",
        "    * batching) 훈련 샘플들의 배치를 만들어 주며 패딩 작업도 이루어 줌. \n",
        "* 전처리 이전에 데이터를 train, val, test로 분리하는 작업을 수행해야 하며, 이후 각 샘플의 단어를 임베딩 벡터로 맵핑해주는 작업(Lookup table)을 수행해야함. "
      ],
      "metadata": {
        "id": "B72FVmFK5Ty-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0.실습 전 참고**"
      ],
      "metadata": {
        "id": "rExhlofu6ugF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.10.0 버전으로 install \n",
        "!pip install torchtext==0.10.0"
      ],
      "metadata": {
        "id": "6WEuf-7N7E_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음의 방식으로 torchtext를 임포트할 수 있음. \n",
        "from torchtext.legacy.data import TabularDataset"
      ],
      "metadata": {
        "id": "3tx-YAke60_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. 훈련 데이터와 테스트 데이터로 분리하기**\n",
        "--- \n",
        "- 이번 실습에서 IMDB 리뷰 데이터를 다운 받아 훈련데이터와 테스트 데이터로 분리함 "
      ],
      "metadata": {
        "id": "2etU0iZEIlfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1RdjZsQIIknr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
      ],
      "metadata": {
        "id": "PSDXKv3mJHs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('IMDb_Reviews.csv', encoding = 'latin1')\n",
        "df.head() # "
      ],
      "metadata": {
        "id": "0JIN_JRjJN5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'전체 샘플의 개수 : {len(df)}')"
      ],
      "metadata": {
        "id": "Fa0yzJ5eJUCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 상위 25,000개의 행은 train_df에 하위 25,000개의 행은 test_df에 저장 \n",
        "train_df = df[:25000]\n",
        "test_df = df[25000:]"
      ],
      "metadata": {
        "id": "4Ep9w_qLKHMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(\"train_data.csv\", index = False)\n",
        "test_df.to_csv(\"test_data.csv\", index = False)"
      ],
      "metadata": {
        "id": "D6N6gw2aKPTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. 필드 정의하기(torchtext.data)**\n",
        "--- \n",
        "* torchtext.data에는 field라는 도구가 제공됨 \n",
        "* 이 필드를 통해 앞으로 어떤 전처리를 할 것인지 정의함. "
      ],
      "metadata": {
        "id": "2cgFQfeSKnBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import data # torchtext.data 임포트"
      ],
      "metadata": {
        "id": "_V7pWwmGKkju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필드 정의\n",
        "TEXT = data.Field(sequential=True,\n",
        "                  use_vocab=True,\n",
        "                  tokenize=str.split,\n",
        "                  lower=True,\n",
        "                  batch_first=True,\n",
        "                  fix_length=20)\n",
        "\n",
        "LABEL = data.Field(sequential=False,\n",
        "                   use_vocab=False,\n",
        "                   batch_first=False,\n",
        "                   is_target=True)"
      ],
      "metadata": {
        "id": "tfiXIWNQK5P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* sequential : 시퀀스 데이터 여부 (True가 기본값) \n",
        "* use_vocab : 단어 집합을 만들 것인지 여부 (True가 기본값) \n",
        "* tokenize : 어떤 토큰화 함수를 사용할 것인지 지정 (str.split이 기본값) \n",
        "* lower : 영어 데이터를 전부 소문자화함 (False가 기본값) \n",
        "* batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부 (False가 기본) \n",
        "* is_target : 레이블 데이터 여부 (False가 기본값) \n",
        "* fix_length : 최대 허용 길이, 그 길이를 맞춰서 패딩(padding) 작업이 진행됨"
      ],
      "metadata": {
        "id": "eLudaGhgNaBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. 데이터셋 만들기**\n",
        "--- "
      ],
      "metadata": {
        "id": "c4XRdmv-OXs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음의 방식으로 torchtext를 임포트할 수 있음. \n",
        "from torchtext.legacy.data import TabularDataset"
      ],
      "metadata": {
        "id": "4Y871ifiObd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = TabularDataset.splits(\n",
        "    path = '.', train='train_data.csv', test = 'test_data.csv', format = 'csv', \n",
        "    fields = [('text',TEXT), ('label',LABEL)], skip_header = True\n",
        ")"
      ],
      "metadata": {
        "id": "Em76R_vbOipp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'훈련 샘플의 개수 : {len(train_data)}')\n",
        "print(f'테스트 샘플의 개수 : {len(test_data)}')"
      ],
      "metadata": {
        "id": "xmKZWySk0oWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vars(train_data[0]))\n",
        "print(train_data.fields.items())"
      ],
      "metadata": {
        "id": "dx9XdxS40wlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TabularDatasets의 fields 인자로 TEXT 필드는 text로 호칭하고, LABEL 필드는 label로 지정하였음. 위 코드를 보았을때, 다음과 같이 구성된 것을 확인할 수 있음. "
      ],
      "metadata": {
        "id": "y4P9X_dT1NvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. 단어 집합(Vocabulary) 만들기**\n",
        "---\n",
        "* 토큰화 전처리를 끝냈다면, 각 단어에 고유한 정수를 맵핑해주는 **정수 인코딩 작업이 필요함 (Integer encoding)** 그리고 전처리를 위해 단어 집합을 만들어 주어야함. "
      ],
      "metadata": {
        "id": "KyuQnsAZ1jrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .build_vocab() 도구를 사용하여 단어 집합을 생성 \n",
        "## min_freq: 단어 집합에 추가 시 단어의 최소 등장 빈도 조건\n",
        "## max_size: 단어 집합의 최대 크기를 지정  \n",
        "TEXT.build_vocab(train_data,min_freq = 10, max_size = 10000)"
      ],
      "metadata": {
        "id": "yIGfFOyL1h-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'단어 집합의 크기: {len(TEXT.vocab)}')"
      ],
      "metadata": {
        "id": "_RQhrbSS2R68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TEXT.vocab.stoi)\n",
        "print(len(TEXT.vocab.stoi)) #  string to int "
      ],
      "metadata": {
        "id": "Oh6089rn2Zdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제로 지정한 단어 집합의 크기는 10,000개 이지만, 생성된 집합의 크기는 총 10,002개로 2개의 토큰이 추가로 생성됨. 이는 `<unk>`와 `<pad` 토큰며, 각각 단어 집합에 없는 단어를 표현하거나 길이를 맞추는 패딩 작업에 사용됨. "
      ],
      "metadata": {
        "id": "XiS1J2Sa2Gef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. 토치텍스트의 데이터로더 만들기**\n",
        "---\n",
        "* 데이터로더는 데이터셋에서 미니 배치만큼 데이터를 로드하게 만들어주는 역할을 함. 토치텍스트에서는 `Iterator`를 사용하여 만들어줌. "
      ],
      "metadata": {
        "id": "9AIMSTCz4Fq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy.data import Iterator"
      ],
      "metadata": {
        "id": "EnvuMf5t4UJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5"
      ],
      "metadata": {
        "id": "20vC4uvK4bk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = Iterator(dataset = train_data, batch_size = batch_size)\n",
        "test_loader = Iterator(dataset=test_data, batch_size = batch_size)\n",
        "\n",
        "print(f'훈련 데이터의 미니 배치 수 : {len(train_loader)}') # 하나의 배치 당 5개 샘플이 존재함\n",
        "print(f'테스트 데이터의 미니 배치 수 : {len(test_loader)}')"
      ],
      "metadata": {
        "id": "g080YA2s49p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader)) # 첫번째 미니 배치"
      ],
      "metadata": {
        "id": "jA82yZsg5GR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(batch))\n",
        "print(batch.text)"
      ],
      "metadata": {
        "id": "kkCoX2C95Yy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. <pad> 토큰이 사용되는 경우**\n",
        "--- \n",
        "* 맨 처음 필드를 정의할때, fix_length를 20이 아니라 150으로 정의한다면, 샘플의 뒷부분에 vocab 상 `<pad>` 토큰의 번호였던 숫자인 1로 채워짐. \n",
        "* 서로 다른 길이의 샘플들을 동일한 길이로 맞춰주는 작업을 패딩 작업(padding)이라고 함. "
      ],
      "metadata": {
        "id": "cx7cfVv0AG4C"
      }
    }
  ]
}