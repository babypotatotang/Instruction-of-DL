{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<u> 01. 선형 회귀(Linear Regression)</u>**\n",
    "\n",
    "이번 챕터에서는 **선형회귀 이론**에 대해 이해하고 파이토치를 이용하여 선형 회귀 모델을 만들어보자.\n",
    "\n",
    "* 데이터에 대한 이해(Data Definition)\n",
    "* 가설 수립 (Hypothesis)\n",
    "* 손실 계산(Compute loss)\n",
    "* 경사하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 데이터에 대한 이해**\n",
    "### (1) 훈련데이터셋과 테스트데이터셋\n",
    "* 훈련 데이터셋(Training Dataset)\n",
    "    * 예측을 위해 사용하는 데이터\n",
    "* 테스트 데이터셋(Test Dataset)\n",
    "    * 학습이 끝난 후, 모델이 얼마나 잘 작동하는지 판별하는 데이터\n",
    "### (2) 훈련 데이터셋의 구성\n",
    "* 모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태(torch.tensor)를 가지고 있음. \n",
    "* 입력과 출력을 각기 다른 텐서에 저장\n",
    "* 입력: x, 출력: y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 가설(Hypothesis) 수립**\n",
    "* **가설(Hypothesis)**\n",
    "    * 머신러닝에서 식을 세울 때 이 식을 가설(Hypothesis)라고 함.  \n",
    "    * 가설을 세우는 방법    \n",
    "        * 임의로 추측해서 세워보는 식\n",
    "        * 경험적으로 알고 있는 식\n",
    "        * 기존의 가설이 아니라고 판단되면 계속 수정해나가는 식 \n",
    "    * 선형 회귀의 가설\n",
    "        $$y=Wx+b\\;or\\;H(x)=Wx+b$$ \n",
    "        * 학습데이터와 가장 잘 맞는 하나의 직선을 찾는 일 \n",
    "        * $W$는 가중치, $b$는 편향을 의미\n",
    "        * 각각 직선의 방정식에서 기울기와 y절편에 해당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 비용함수(Cost Function)에 대한 이해**\n",
    "* 용어 정리\n",
    "    * 아래의 용어는 모두 같은 의미를 가짐\n",
    "    * cost function (비용)\n",
    "    * loss function (손실)\n",
    "    * error function (오차)\n",
    "    * objective function (목적)\n",
    "* 오차\n",
    "    * 정의: 실제값-예측값\n",
    "    * 데이터에 따라 오차값이 +가 되었다가 -되었다가 하므로 제대로된 오차의 크기를 측정할 수 없음. \n",
    "    * **MSE(평균 제곱 오차)**\n",
    "        * 제대로된 오차를 측정하기위해 도입되었음. \n",
    "        * 오차 제곱합에 대한 평균\n",
    "        * MSE에 의한 비용함수   \n",
    "            $$ cost(W,b)=  \\frac{1}{n} \\sum_{i=1}^n   \\begin{bmatrix}  y^{(i)}-H(x^{(i)}) \\end{bmatrix}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 옵티마이저 - 경사하강법(Gradient Descent)**\n",
    "* **최적화 알고리즘**이라고도 불림 \n",
    "* 최적화 알고리즘을 통해 적절한 $W$와 $b$를 찾아내는 과정은 머신러닝에서 **학습(training)**이라고 부름. \n",
    "* 가장 기본적인 옵티마이저 알고리즘인 경사하강법에 대해서 다룸. \n",
    "\n",
    "### **(1) 경사하강법**\n",
    "* 비용함수를 **미분**하여 현재 $W$에서의 접선의 기울기를 구하고, **접선의 기울기가 낮은 방향으로** $W$의 값을 변경하는 작업을 반복 \n",
    "* 접선의 기울기가 낮다 => 비용함수의 값이 가장 작은 경우 \n",
    "* 기울기\n",
    "    $$기울기 = \\frac{ \\partial(W)}{\\partial(W)} $$\n",
    "    * 기울기가 음수: $W$의 값이 증가\n",
    "        $$ W := W- \\alpha \\times (음수 기울기) = W + \\alpha  \\times (양수 기울기) $$\n",
    "        * 기울기가 음수면 $W$의 값이 증가하는데 이는 결과적으로 접선의 기울기가 0인 방향으로 조정됨\n",
    "    * 기울기가 양수 : $W$의 값이 감소 \n",
    "    $$ W := W- \\alpha \\times (양수 기울기)$$\n",
    "    * 기울기가 양수면 $W$의 값이 감소하게 되는데, 이는 결과적으로 기울기가 0인 방향으로 조정됨. \n",
    "\n",
    "    * (결론) 접선의 기울기가 음수거나 양수일때 모두 접선의 기울기가 0인 방향으로(_비용이 최소가 되는 방향_) $W$의 값이 조정됨. \n",
    "        $$W := W- \\alpha  \\frac{ \\partial W}{ \\partial } cost(W)$$\n",
    "* 학습률\n",
    "    * $\\alpha$\n",
    "    * $W$의 값을 변경할때, 얼마나 크게 변경할지를 결정\n",
    "    * $W$를 그래프의 한 점으로 보고 접선의 기울기가 0일때까지 경사를 따라 내겨람. \n",
    "    * 학습률의 값을 무작정 크게 하면 접선의 기울기가 최소값이 되는 $W$를 찾기 어려움. \n",
    "        * $W$의 값을 찾아가는 것이 아니라 $W$의 값이 **발산**하게됨. \n",
    "    * 학습률의 값을 지나치게 작은 값을 가지면 학습 속도가 느려짐. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. 파이토치로 선형회귀 구현하기**\n",
    "### **(1) 기본 셋팅**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19761fdb110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#현재 실습하고 있는 파이썬 코드를 재실행해도 같은 결과가 나오도록 랜덤시드(random seed) \n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2) 변수 선언**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=torch.FloatTensor([[1],[2],[3]])\n",
    "y_train=torch.FloatTensor([[2],[4],[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 각 train 값이 출력, 크기 출력\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3) 가중치와 평향의 초기화**\n",
    "* 선형회귀의 목표\n",
    ": 가장 잘 맞는 직선을 정의하는 $W$와 $b$의 값을 찾는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W=torch.zeros(1,requires_grad=True)\n",
    "print(W)\n",
    "\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가중치 W와 편향 b를 0으로 초기화\n",
    "* **requires_grad=True**를 통해 학습을 통해 값이 변경되는 변수임을 명시\n",
    "* 현재 직선의 방정식은 다음과 같음. \n",
    "    $$ y=0 \\times x  + 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4) 가설 세우기**\n",
    "* 파이토치 코드 상으로 직선의 방정식에 해당되는 가설 선언\n",
    "$$H(x) = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis=x_train*W+b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(5) 비용함수 선언하기**\n",
    "* 파이토치 코드 상으로 선형 회귀의 비용 함수에 해당되는 **평균제곱오차** 선언\n",
    "$$cost(W,b)= \\frac{1}{n}  \\sum_{i=1}^n  \\begin{bmatrix} y^{(i)}-H(x^{(i)}) \\end{bmatrix} ^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost=torch.mean((y_train-hypothesis)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(6) 경사하강법 구현하기**\n",
    "* 'SGD'는 경사하강법의 일종\n",
    "* lr은 학습률(learning rate)를 의미\n",
    "* 학습 대상인 W와 b가 SGD의 입력이 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.SGD([W,b],lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* optimizer.zero_grad()\n",
    "    * 위 함수 실행을 통해, 미분으로 얻은 기울기를 0으로 초기화\n",
    "    * 기울기를 초기화해야만 새오룬 가중치 편향에 대해 새로운 기울기를 구할 수 있음. \n",
    "* cost.backward()\n",
    "    * 가중치 W와 편향 b에 대한 기울기가 계산됨. \n",
    "* optimizer.step()\n",
    "    * 인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 학습률 0.01을 곱하여 빼주면서 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(7) 전체 코드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1999 W:0.187, b=0.080 Cost: 18.666666\n",
      "Epoch  100/1999 W:1.746, b=0.578 Cost: 0.048171\n",
      "Epoch  200/1999 W:1.800, b=0.454 Cost: 0.029767\n",
      "Epoch  300/1999 W:1.843, b=0.357 Cost: 0.018394\n",
      "Epoch  400/1999 W:1.876, b=0.281 Cost: 0.011366\n",
      "Epoch  500/1999 W:1.903, b=0.221 Cost: 0.007024\n",
      "Epoch  600/1999 W:1.924, b=0.174 Cost: 0.004340\n",
      "Epoch  700/1999 W:1.940, b=0.136 Cost: 0.002682\n",
      "Epoch  800/1999 W:1.953, b=0.107 Cost: 0.001657\n",
      "Epoch  900/1999 W:1.963, b=0.084 Cost: 0.001024\n",
      "Epoch 1000/1999 W:1.971, b=0.066 Cost: 0.000633\n",
      "Epoch 1100/1999 W:1.977, b=0.052 Cost: 0.000391\n",
      "Epoch 1200/1999 W:1.982, b=0.041 Cost: 0.000242\n",
      "Epoch 1300/1999 W:1.986, b=0.032 Cost: 0.000149\n",
      "Epoch 1400/1999 W:1.989, b=0.025 Cost: 0.000092\n",
      "Epoch 1500/1999 W:1.991, b=0.020 Cost: 0.000057\n",
      "Epoch 1600/1999 W:1.993, b=0.016 Cost: 0.000035\n",
      "Epoch 1700/1999 W:1.995, b=0.012 Cost: 0.000022\n",
      "Epoch 1800/1999 W:1.996, b=0.010 Cost: 0.000013\n",
      "Epoch 1900/1999 W:1.997, b=0.008 Cost: 0.000008\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#현재 실습하고 있는 파이썬 코드를 재실행해도 같은 결과가 나오도록 랜덤시드(random seed) \n",
    "torch.manual_seed(1)\n",
    "x_train=torch.FloatTensor([[1],[2],[3]])\n",
    "y_train=torch.FloatTensor([[2],[4],[6]])\n",
    "\n",
    "W=torch.zeros(1,requires_grad=True)\n",
    "b=torch.zeros(1,requires_grad=True)\n",
    "\n",
    "optimizer=optim.SGD([W,b],lr=0.01)\n",
    "\n",
    "nb_epochs=1999 # 경사하강법을 반복하는 횟수\n",
    "for epoch in range(nb_epochs+1):\n",
    "    \n",
    "    hypothesis=x_train*W+b # 가설 계산 \n",
    "    cost=torch.mean((y_train-hypothesis)**2) # cost 계산\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch%100==0:\n",
    "        print('Epoch {:4d}/{} W:{:.3f}, b={:.3f} Cost: {:.6f}'.format(epoch,nb_epochs,W.item(),b.item(),cost.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.Optimizer.zero_grad()가 필요한 이유**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값: 2.0\n",
      "수식을 w로 미분한 값: 4.0\n",
      "수식을 w로 미분한 값: 6.0\n",
      "수식을 w로 미분한 값: 8.0\n",
      "수식을 w로 미분한 값: 10.0\n",
      "수식을 w로 미분한 값: 12.0\n",
      "수식을 w로 미분한 값: 14.0\n",
      "수식을 w로 미분한 값: 16.0\n",
      "수식을 w로 미분한 값: 18.0\n",
      "수식을 w로 미분한 값: 20.0\n",
      "수식을 w로 미분한 값: 22.0\n",
      "수식을 w로 미분한 값: 24.0\n",
      "수식을 w로 미분한 값: 26.0\n",
      "수식을 w로 미분한 값: 28.0\n",
      "수식을 w로 미분한 값: 30.0\n",
      "수식을 w로 미분한 값: 32.0\n",
      "수식을 w로 미분한 값: 34.0\n",
      "수식을 w로 미분한 값: 36.0\n",
      "수식을 w로 미분한 값: 38.0\n",
      "수식을 w로 미분한 값: 40.0\n",
      "수식을 w로 미분한 값: 42.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w=torch.tensor(2.0,requires_grad=True)\n",
    "\n",
    "nb_epochs=20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    z=2*w\n",
    "    z.backward()\n",
    "    print('수식을 w로 미분한 값: {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 계속해서 미분갓인 2가 누적되는 것을 확인\n",
    "* optimizer.zero_grad()를 통해 미분값을 계속 0으로 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. torch.manual_seed()를 하는 이유**\n",
    "* torch.manual_seed()를 사용하여 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 3일때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 3일때')\n",
    "for i in range(1,3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 5일때\n",
      "tensor([0.8303])\n",
      "tensor([0.1261])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "print('랜덤 시드가 5일때')\n",
    "for i in range(1,3):\n",
    "    print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 다시 3일때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 다시 3일때')\n",
    "for i in range(1,3):\n",
    "    print(torch.rand(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33bb709aa52d0f1c012e153b74c2c585d1723b1d162db1a778a7381b9d9efdce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
