{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **04. Linear Regression with nn.Module**\n",
    "* 파이토치에서 이미 **구현되어 제공되는 함수**를 불러오는 것으로 쉽게 선형회귀모델을 구현. \n",
    "* 파이토치에서 사용\n",
    "  * 선형 회귀 모델: `nn.Linear()`\n",
    "  * 평균 제곱오차: `nn.functional.mse_loss()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 단순 선형 회귀 구현하기**\n",
    "### **(1) 필요한 패키지 import 및 초기화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x177778491f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2) 훈련데이터 선언**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w=2, b=0인 모델\n",
    "x_train=torch.FloatTensor([[1],[2],[3]])\n",
    "y_train=torch.FloatTensor([[2],[4],[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 예측 모델의 가중치(w)를 2, 편향(b)을 0으로 고정하였음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3) 훈련데이터 선언**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 선언 및 초기화\n",
    "model=nn.Linear(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하나의 입력 $x$에 대해서 하나의 출력 $y$를 예측하는 모델\n",
    "* input_dim은 1, output_dim은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters())) #랜덤 초기화 됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `model.parameters` 함수로 모델의 가중치(`w`)와 편향(`b`)을 확인할 수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4) 최적화: 경사하강법으로**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "nb_epochs=2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    # H(x)\n",
    "    prediction=model(x_train)\n",
    "    \n",
    "    #cost\n",
    "    cost=F.mse_loss(prediction,y_train) # 파이토치에서 제공하는 평균 제곱 오차 함수 \n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward() \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100==0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch,nb_epochs,cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 다중 선형 회귀 구현하기**\n",
    "* 다중선형회귀모델을 구현하는 것으로, 3개의 입력과 1개의 출력이 있음. \n",
    "* 가설 수식\n",
    "  * 3개의 `x`로부터 1개의 `y`를 예측하는 문제\n",
    "  * 수식: $H(x)=w_1x_1+w_2x_2+w_3x_3+b$\n",
    "  \n",
    "### **(1) 필요한 패키지 import 및 초기화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x177778491f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2) 훈련데이터 선언**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터\n",
    "x_train=torch.FloatTensor([[73,80,75],\n",
    "                           [93,88,93],\n",
    "                           [89,91,90],\n",
    "                           [96,98,100],\n",
    "                           [73,66,70]])\n",
    "y_train=torch.FloatTensor([[152],[185],[180],[196],[142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3) 모델 선언 및 초기화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2710], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model=nn.Linear(3,1)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4) 최적화: 경사하강법으로**\n",
    "#### **(4-1) optim 선언**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4-2) 최적화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 31667.599609\n",
      "Epoch  100/2000 Cost: 0.225993\n",
      "Epoch  200/2000 Cost: 0.223911\n",
      "Epoch  300/2000 Cost: 0.221941\n",
      "Epoch  400/2000 Cost: 0.220059\n",
      "Epoch  500/2000 Cost: 0.218271\n",
      "Epoch  600/2000 Cost: 0.216575\n",
      "Epoch  700/2000 Cost: 0.214950\n",
      "Epoch  800/2000 Cost: 0.213413\n",
      "Epoch  900/2000 Cost: 0.211952\n",
      "Epoch 1000/2000 Cost: 0.210559\n",
      "Epoch 1100/2000 Cost: 0.209230\n",
      "Epoch 1200/2000 Cost: 0.207967\n",
      "Epoch 1300/2000 Cost: 0.206762\n",
      "Epoch 1400/2000 Cost: 0.205618\n",
      "Epoch 1500/2000 Cost: 0.204529\n",
      "Epoch 1600/2000 Cost: 0.203481\n",
      "Epoch 1700/2000 Cost: 0.202486\n",
      "Epoch 1800/2000 Cost: 0.201539\n",
      "Epoch 1900/2000 Cost: 0.200634\n",
      "Epoch 2000/2000 Cost: 0.199770\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    #H(x)\n",
    "    prediction=model(x_train) \n",
    "    # (동일) prediction=model.forward(x_train)\n",
    "\n",
    "    #cost\n",
    "    cost=F.mse_loss(prediction,y_train) # 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100==0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch,nb_epochs,cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(5) 모델 테스트 및 파라메터 확인**\n",
    "#### **(5-1) 모델 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 [73,80,75]일때의 예측값: tensor([[151.2306]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73,80,75] 선언\n",
    "new_var=torch.FloatTensor([[73,80,75]])\n",
    "# 입력한 값에 대한 예측값을 pred_y에 저장\n",
    "pred_y=model(new_var)\n",
    "print('훈련 후 입력이 [73,80,75]일때의 예측값:',pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [73,80,75]의 임의의 입력을 넣어 `pred_y` 값 확인\n",
    "* 해당 임의의 입력은 훈련에 사용되었던 데이터로 당시 y값 152와 비교하면 근사한 값을 예측했음을 알 수 있음. \n",
    "#### **(5-2) 파라메터 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2802], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33bb709aa52d0f1c012e153b74c2c585d1723b1d162db1a778a7381b9d9efdce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
