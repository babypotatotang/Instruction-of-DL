{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNjOVloslxYop4nhpBxDnE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babypotatotang/Introduction-to-DeepLearning/blob/main/6.%20Artifical%20Neural%20Network/10.%20%EA%B8%B0%EC%9A%B8%EA%B8%B0%20%EC%86%8C%EC%8B%A4(Gradient_Vanishing)%EA%B3%BC%20%ED%8F%AD%EC%A3%BC(Exploding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "깊은 인공신경망을 학습하다보면 **역전파 과정**에서 입력층으로 갈수록 기울기 (Gradient)가 점차적으로 작아지는 현상이 발생할 수 있음. 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 최적의 모델을 찾을 수 없음.  이를 **기울기 소실(Gradient Vanishing)**이라고 함. \n",
        "\n",
        "#1. ReLU와 ReLU의 변형들 \n",
        "--- \n",
        "* 시그모이드 함수를 사용하면 입력의 절대값이 클 경우 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워짐. 그래서 역전파 과정에서 전파시킬 기울기가 점차 사라지며 입력층 방향으로 갈수록 제대로 역전파가 되지 않아 기울기 소실 문제가 발생함. \n",
        "* 기울기 소실을 완화하는 가장 간단한 방법은 은닉층의 활성화 함수를 **ReLU(Leaky ReLU)**로 바꾸는 것임. \n",
        "\n",
        "### points\n",
        "* 은닉층에서 시그모이드 함수를 사용하지 말것. \n",
        "* Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않는 Dying ReLU문제를 해결할 수 있음. \n",
        "\n",
        "#2. 가중치 초기화 (Weight Initialization) \n",
        "--- \n",
        "같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라 모델의 훈력 결과가 달라짐. 즉, **가중치 초기화만 적절히 해줘도** 기울기 소실 문제를 방지할 수 있음. \n",
        "## 1. 세이비어 초기화 (Xavier Initialization) \n",
        "* 글로럿 초기화(Glorot Initialization)이라고도 함. \n",
        "* 이 방법은 균등 분포 또는 정규 분포로 초기화할때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세움. 이천 층의 뉴런의 개수를 $n_{in}$, 다음 층의 뉴런의 개수를 $n_{out}$이라고 하자. 균등 분포를 사용하여 가중치를 초기화할 경우 다음과 같은 균등 분포 범위를 사용함. \n",
        "$$W \\sim Uniform(-  \\sqrt{\\frac{6}{n_{in}+n_{out}}}, + \\sqrt{\\frac{6}{n_{in}+n_{out}}}) $$ \n",
        "* 이 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목받거나, 뒤쳐지는 것을 막음. 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같이 S자 형태인 활성화 함수와 함께 사용하면 좋은 성능을 보임. \n",
        "\n",
        "##2. He 초기화 (He Initialization) \n",
        "* 세이비어 초기화와 유사하게 정규 분포와 균등 분포 두 가지 경우로 나뉨. 다만 He 초기화는 세이비어 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않음. \n",
        "$$ W \\sim Uniform(-\\sqrt{(\\frac{6}{n_{in}}}, + \\sqrt{(\\frac{6}{n_{in}}}) $$\n",
        "\n",
        "* ReLU 계열 함수를 사용할 경우 He 초기화 방법이 효율적임. \n",
        "* 일반적으로 ReLU + He 초기화 방법이 보편적으로 사용됨. \n",
        "\n",
        "# 3. 배치 정규화(Batch Normalization) \n",
        "--- \n",
        "인공 신경망의 각 층에 들어가는 입력과 분산으로 정규화하여 학습을 효율적으로 만들어줌. \n",
        "\n",
        "## 1. 내부 공변량 변화(Internal Covariate Shift) \n",
        "* 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상\n",
        "* 이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌면서 현재 층에 전달되는 현재 층에 전달되는 입력데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생함. \n",
        "* 층마다 입력의 분포가 달라지는 점을 개선\n",
        "\n",
        "## 2. 배치 정규화 (Batch Normalization) \n",
        "* 각 층에서 활성화 함수를 통과하기 전에 수행. \n",
        "* (요약) 입력에 대해 평균을 0으로 만들고, 정규화 진행. 정규화된 데이터에 대해 스케일과 시프트 수행.\n",
        "\n",
        "## 3. 한계 \n",
        "### 미니 배치 크기에 의존적이다. \n",
        "* 너무 작은 배치 크기에서 잘 동작하지 않음. 단적으로 배치 크기를 1로 하게 되면 분산은 0이 됨. 배치 정규화를 적용할때는 작은 미니 배치 보다는 크기가 어느 정도 되는 미니 배치에서 하는 것이 좋음. \n",
        "\n",
        "### RNN에 적용하기 어렵다. \n",
        "\n",
        "* RNN은 각 시점마다 다른 통계치를 가짐. 이는 RNN에 배치 정규화를 적용하는 것을 어렵게 만들었음. \n",
        "* 하지만 RNN에서도 작동하는 layer normalization, 층 정규화 방법이 존재함. \n",
        "\n",
        "# 4. 층 정규화 (Layer Normalization) \n",
        "![](https://wikidocs.net/images/page/61375/%EC%B8%B5%EC%A0%95%EA%B7%9C%ED%99%94.PNG)\n",
        "* 데이터 별로 정규화 \n",
        "* 미니배치 수만큼의 평균과 분산을 계산함. "
      ],
      "metadata": {
        "id": "WX5WyU-9h-lp"
      }
    }
  ]
}