{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXpCETFeJUZY3n1TW7EQo3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babypotatotang/Introduction-to-DeepLearning/blob/main/6.%20Artifical%20Neural%20Network/09.%20%EA%B3%BC%EC%A0%81%ED%95%A9%EC%9D%84%20%EB%A7%89%EB%8A%94%20%EB%B0%A9%EB%B2%95%EB%93%A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Overfitting` 모델이 학습 데이터를 불필요할정도로 과하게 암기하여, 훈련데이터에 포함된 노이즈까지 학습한 상태 \n",
        "* 학습데이터에 모델이 과적합되면 모델의 성능을 떨어트림 \n",
        "* 모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라고, 새로운 데이터(검증, 테스트)에 대해서는 제대로 동작하지 않음. \n",
        "* 이번에는 모델의 과적합을 막을 수 있는 방법에 대해 논의해보자 "
      ],
      "metadata": {
        "id": "Ye_QDKSuvJ7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. 데이터의 양을 늘리기 \n",
        "---\n",
        "* 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합 현생이 발생할 확률이 늘어남. 그렇기 때문에 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있음. \n",
        "* 데이터의 양이 적을 경우 의도적으로 기존의 데이터를 변형하고 추가하여 (data augmentation) 데이터의 양을 늘리기도 함. \n",
        "    * vision의 경우 **이미지를 돌리거나 노이즈를 추가하고, 일부분을 수정하는 등으로** 데이터를 증식시킴. \n",
        "\n",
        "\n",
        "#2. 모델의 복잡도 줄이기 \n",
        "--- \n",
        "* 인공 신경망의 복잡도는 은닉층(hidden layer)의 수나 매개변수의 수 등으로 결정됨. 과적합 현상이 포착되었을때, 인공신경망의 복잡도를 줄이는 것으로, 과적합 현상을 막을 수 있음. "
      ],
      "metadata": {
        "id": "PJZtKVVHvvMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "M62BbPqpxIAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPL3RC3Suq-d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "아래의 구조는 3개의 선형 layer(linear)을 가지고 있음. \n",
        "'''\n",
        "\n",
        "class Architecture1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Architecture1, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "위 구조에서 과적합 현상이 발생했다면, \n",
        "아래와 같이 layer을 줄이는 방식으로 복잡도를 줄일 수 있음. \n",
        "\n",
        "\n",
        "인공신경망에서 모델에 있는 매개변수의 수를 모델의 수용력(capacity)라고 함. \n",
        "'''\n",
        "\n",
        "class Architecture1(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(Architecture1, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "KZU7R5bZw-Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 가중치 규제 (Regularization) 적용하기 \n",
        "---\n",
        "복잡한 모델이 간단한 모델보다 과적합될 가능성이 높음. 복잡한 모델을 더 간단하게 만드는 방법 중 **가중치 규제**가 있음. \n",
        "\n",
        "* L1 규제: 가중치 w들의 절대값 합계를 비용함수에 추가\n",
        "* L2 규제: 모든 가중치 w들의 제곱합을 비용함수에 추가 \n",
        "\n",
        "* L1 규제는 기존의 비용함수에 모든 가중치에 대해 $λ|W|$를 더한 값을 비용함수로 하고 , L2 규제는 기존의 비용함수에 모든 가중치에 대해 $ \\frac{1}{2}\\lambda\\omega^2$를 더한 값을 비용함수로 함. \n",
        "    * $\\lambda$가 크다면 모델이 훈련 데이터에 대해 적합한 매개변수를 찾는게 아니라, 규제를 위해 추가된 항을 작게 유지하는데 우선한다는 의미를 가짐. \n",
        "    * 가중치 $\\lambda$에 따라 L1, L2 norm이 모델에 미치는 영향이 달라짐. \n",
        "\n",
        "### L1 규제와 L2 규제의 사용 \n",
        "* L1 규제의 경우, 비용함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야하기 때문에, 가중치 w의 값들은 0 또는 0에 가까이 작아져야하므로 어떤 특성은 모델링에 사용되지 않을 수 있음. \n",
        "\n",
        "\n",
        "* L2 규제의 경우 가중치 제곱을 최소화하므로, **완전히 0이 되는 것이 아니라 0에 가까워지는 경향을 띔**. 가중치 감쇠, weight decay라고도 부름. \n",
        "\n",
        "* 따라서 **어떤 특성들이 모델에 영향을 주고 있는지 판단하고자** 한때는 L1 규제가 더 유용하지만, 경험적으로 L2 규제가 더 잘 동작하므로 L2 규제를 사용하는 것이 좋음. \n",
        "\n",
        "\n",
        "# 4. 드롭아웃 (Dropout)\n",
        "--- \n",
        "![](https://wikidocs.net/images/page/60751/%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83.PNG)\n",
        "* 학습과정에서 신경망의 일부를 사용하지 않는 방법. \n",
        "* 드롭아웃 비율을 0.5로 학습한다면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용함. \n",
        "* 드롭아웃의 경우 학습시에만 사용하는 것이 일반적임. 학습 과정에서 네트워크가 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고, 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지할 수 있음.  "
      ],
      "metadata": {
        "id": "oTSqwa3ixvbE"
      }
    }
  ]
}