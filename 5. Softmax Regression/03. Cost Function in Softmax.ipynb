{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **03. 소프트맥스 회귀의 비용함수 구현하기**\n",
    "---\n",
    "* 이번 챕터에서 소프트맥스 회귀의 비용함수를 구현해보자. \n",
    "* 모든 실습은 아래 초기화 과정이 진행되었다고 가정함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2594337a230>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **01. 파이토치로 소프트맥스의 비용 함수 구현하기(low-level)**\n",
    "* 다음 3개의 원소를 가진 벡터 텐서를 정의하고, 이 텐서를 통해 소프트맥스 함수를 이해해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 텐서 선언 \n",
    "z = torch.FloatTensor([1,2,3])\n",
    "\n",
    "# softmax 함수 실행 \n",
    "hypothesis = F.softmax(z, dim = 0)\n",
    "\n",
    "print(hypothesis) # 3개의 원소 값이 0과 1사이의 값을 가지는 벡터로 변환됨.\n",
    "print(hypothesis.sum()) # 이 원소들의 합은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 3 x 5 행렬 크기를 가진 텐서를 만들었음.\n",
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "hypothesis = F.softmax(z, dim=1) # 두번째 차원에 대해서 softmax 함수 적용 \n",
    "\n",
    "print(hypothesis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# 각 샘플에 대한 임의의 테이블 생성\n",
    "y = torch.randint(5,(3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원-핫 인코딩 생성 \n",
    "# 모든 원소가 0의 값을 가진 3 * 5 텐서 생성 \n",
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **원-핫 인코딩 동작 원리**\n",
    "    *  `y_one_hot = torch.zero_like(hypothesis)`\n",
    "        * 모든 원소가 0의 값을 가진 3 x 5 텐서를 만들어 `y_one_hot`에 저장\n",
    "    *  `y_one_hot.scatter_(1, y.unsqueeze(1),1)`\n",
    "        * `y.unsqueeze(1)` 를 하면 (3,)의 크기를 가졌던 y 텐서는 (3 x 1) 텐서가 됨. \n",
    "        * `scatter`의 첫번째 인자로 `dim=1`에 대해서 수행하라고 알려주고, 세번째 인자에 숫자 1을 넣어 `y_unsquezze(1)`이 알려주는 위치에 1을 넣어주어 one-hot encoding\n",
    "    * 연산 뒤에 _를 붙이는 **In-Place Operation**을 통해 `y_one_hot`에 값을 넣어줌. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이제 비용함수 구현\n",
    "* 비용함수 식은 다음과 같음. \n",
    "$$cost(W) =  -\\frac{1}{n} \\sum_{i=1}^n  \\sum_{j=1}^k y_j^i log(p_j^i)$$\n",
    "* 마이너스 부호를 뒤로 빼면 다음 식과 동일함. \n",
    "$$cost(W) =  \\frac{1}{n} \\sum_{i=1}^n  \\sum_{j=1}^k y_j^i * (-log(p_j^i))$$\n",
    "* 이 식에서 $\\sum_{j=1}^{k}$ 는 `sum(dim=1)`으로 구현하고, $ \\frac{1}{n} \\sum_{i=1}^{n}$ `mean()`으로 구현함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6682, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.파이토치로 소프트맥스의 비용함수 구현하기(High-Level)**\n",
    "* 하이-레벨로 구현하는 방법에 대해서 알아보자. \n",
    "### **01. F.softmax() + torch.log() = F.log_softmax()**\n",
    "* 앞서 low-level에서 함수의 결과에 로그를 씌울때 다음과 같이 사용하였음. : `torch.log(F.softmax())`\n",
    "\n",
    "* 파이토치에서는 두 개의 함수를 결합한 `F.log_softmax()`라는 도구를 제공함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **02. F.log_softmax() + F.nll_loss() = F.cross_entropy()**\n",
    "* 로우-레벨로 구현한 비용함수는 다음과 같음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6682, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low Level \n",
    "# 첫번째 수식\n",
    "(y_one_hot * -torch.log(F.softmax(z,dim=1))).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6682, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두번째 수식\n",
    "(y_one_hot * -F.log_softmax(z,dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6682, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세번째 수식\n",
    "F.nll_loss(F.log_softmax(z,dim=1),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 여기서 nll이란 `Negative Log Likelihood`의 약자\n",
    "* `nll_loss`는 `F.log_softmax()`를 수행한 후에 남은 수식들을 수행함. \n",
    "* 이를 더 간단히 하면 `F.cross_entropy()`는 `F.log_softmax()`와 `F.null_loss()`를 포함하고 있음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6682, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 네번째 수식\n",
    "F.cross_entropy(z,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`F.cross_entropy`는 비용 함수에 소프트맥스 함수까지 포함하고 있음.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33bb709aa52d0f1c012e153b74c2c585d1723b1d162db1a778a7381b9d9efdce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
