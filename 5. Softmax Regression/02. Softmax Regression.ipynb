{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **02. 소프트맥스 회귀 이해하기**\n",
    "---\n",
    "* 소프트맥스 회귀를 통해 3개 이상의 선택지 중에서 1개를 고르는 **다중 클래스 분류**를 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **01. 다중 클래스 분류(Multi-Class Classification)**\n",
    "* (참고) 이번 챕터에서 입력은 $X$, 가중치는 $W$, 편향은 $B$, 출력은 $\\widehat{Y}$를 의미 \n",
    "    * $\\widehat{Y}$은 예측값이라는 의미를 가지고 있으므로 가설식에서 $H(X)$ 대신 사용되기도 함. \n",
    "* 세 개 이상의 답 중 하나를 고르는 문재를 다중 클래스 분류라고 함. \n",
    "* 꽃받침 길이, 꽃받침 넓이, 꽃받침 길이, 꽃잎 넓이라는 4개의 특성(feature)로 부터 3개의 붓꽃 품종 중 어떤 품종인지 예측하는 문제\n",
    "\n",
    "\n",
    "|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|5.1|3.5|1.4|0.2|setosa|\n",
    "|4.9|3.0|1.4|0.2|setosa|\n",
    "|5.8|2.6|4.0|1.2|versicolor|\n",
    "|6.7|3.0|5.2|2.3|virginica|\n",
    "|5.6|2.8|4.9|2.0|virginica|\n",
    "\n",
    "\n",
    "* 위 **붗꽃 품종 분류하기 문제**를 어떻게 풀지 고민하기 위해 앞서 배운 **로지스틱회귀의** 이진 분류를 복습 \n",
    "\n",
    "### **1. 로지스틱 회귀**\n",
    "* 시그모이드 함수는 예측값을 0과 1 사이의 값으로 만듦. \n",
    "* 스팸 메일 분류기를 로지스틱 회귀로 구현하였을때, 출력이 0.75라면 이메일이 스팸일 확률 75%라는 의미\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/59427/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80.PNG)\n",
    "* 가설: $H(X)=sigmoid(WX+B)$\n",
    "\n",
    "### **2. 소프트맥스 회귀**\n",
    "* 소프트맥스 회귀는 각 클래스, 즉 각 선택지마다 소수 확률을 할당함. \n",
    "* 이때 총 확률의 합은 1이 되어야 하고, 각 확률은 각 선택지가 정답일 확률을 표현함. \n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/59427/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%ED%9A%8C%EA%B7%80.PNG)\n",
    "* 소프트맥스 회귀는 선택지의 개수만큼 차원을 가지는 벡터를 만들고, 해당 벡터가 벡터의 모든 원소의 합이 1이 되도록 원소들의 값을 변환시키는 어떤 함수를 지나게 만들어야함. \n",
    "* 빨간색의 **어떤 함수 ?**를 지나 원소의 총 합이 1이 되도록 원소의 값이 변환되는 모습을 보여줌. \n",
    "* 이 함수를 **소프트맥스(softmax)** 함수라고 함. \n",
    "* 가설: $H(x)=softmax(WX+B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **02. 소프트맥스 함수(Softmax Function)**\n",
    "* 정답지(클래스)의 총 개수를 k라고 할때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정함. \n",
    "### **01. 소프트맥스 함수의 이해**\n",
    "* k차원의 벡터에서 i번째 원소를 $z_i$, i번째 클래스가 정답일 확률을 $p_i$라고 한다면 소프트맥스 함수는 $p_i$를 다음과 같이 정의\n",
    "$$p_i= \\frac{e^{z_{i}}}{ \\sum_{j=1}^k e^{z_j} }$$\n",
    "* 위에서 풀어야하는 문제에 소프트맥스 함수를 적용\n",
    "$$softmax(z)=[ \\frac{e^{z_1}}{ \\sum_{j=1}^k e^{z_j} } \\:\\frac{e^{z_2}}{ \\sum_{j=1}^k e^{z_j} }\\:\n",
    "\\frac{e^{z_3}}{ \\sum_{j=1}^k e^{z_j} }] = [p_1, p_2, p_3] =  \\widehat{y}$$\n",
    "* $p_1, p_2, p_3$ 각각은 1번 클래스가 정답일 확률, 2번 클래스가 정답일 확률, 3번 클래스가 정답일 확률을 나타내며 각각 0과 1사이의 값으로 총 합은 1이 됨. \n",
    "    * 여기서 분류하고자 하는 클래스는 virginica(1), setosa(2), versicolor(3)임.\n",
    "* 분류하고자 하는 클래스가 k개 일 때, k차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 변경하여 다시 k차원의 벡터를 리턴함. \n",
    "### **02. 그림을 통한 이해**\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/35476/softmax1_final_final.PNG)\n",
    "* **소프트맥스 함수의 입력으로 어떻게 바꿀까?**\n",
    "    * 하나의 샘플 데이터는 4개의 독립 변수 $x$를 가지는데 이는 모델이 **4차원 벡터**를 입력으로 받음을 의미 (feature는 4개, class는 3개)\n",
    "    * 소프트맥스 함수의 입력으로 사용되는 벡터는 벡터의 차원이 분류하고자 하는 클래스의 개수가 되어야하므로 **3차원 벡터**로 변환되어야함. \n",
    "\n",
    "\n",
    "    ![](https://wikidocs.net/images/page/35476/softmaxbetween1and2.PNG)\n",
    "    * 소프트맥스 함수의 입력 벡터 $z$의 차원 수 만큼 결과값이 나오도록 가중치 곱을 진행. \n",
    "    * 화살표는 총 4 * 3 =12, 12개 이며 전부 다른 가중치를 가지고, 학습 과정에서 **점차적으로 오차를 최소화하는 가중치로 값이 변경됨.** \n",
    "* **오차 계산 방법은 어떻게?**\n",
    "    * 소프트맥스 함수의 출력은 분류하고자하는 **클래스의 개수만큼** 차원을 가지는 벡터로 각 원소는 0과 1사이의 값을 가짐. \n",
    "    * 이는 특정 클래스가 정답일 확률을 나타냄. \n",
    "    * 소프트맥스 회귀에서 실제값은 원-핫 벡터로 표현함. \n",
    "\n",
    "\n",
    "    ![](https://wikidocs.net/images/page/35476/softmax2_final.PNG)\n",
    "    * 각 실제값의 정수 인코딩은 1,2,3이 되고 이에 원-핫 인코딩을 수행하여 실제값을 원-핫 벡터로 수치화\n",
    "\n",
    "\n",
    "    ![](https://wikidocs.net/images/page/35476/softmax4.PNG)\n",
    "\n",
    "    \n",
    "    ![](https://wikidocs.net/images/page/35476/softmax5.PNG)\n",
    "    \n",
    "    * 예를 들어 실제값이 setosa(2)라면 원-핫벡터는 [0 1 0]임. \n",
    "    * 이 경우, 예측값과 실제값의 오차가 0이 되는 경우는 소프트맥스 함수의 결과가 [0 1 0]이되는 경우이므로, 두 벡터의 오차를 계산하기 위해\n",
    "    소프트맥스 회귀는 비용함수로 **크로스 엔트로피 함수**를 사용하여 가중치 업데이트\n",
    "\n",
    "    * 소프트맥스 회귀에서 예측값을 구하는 과정을 벡터와 행렬 연산으로 표현하면 아래와 같음. \n",
    "\n",
    "\n",
    "    ![](https://wikidocs.net/images/page/59427/%EA%B0%80%EC%84%A4.PNG)\n",
    "\n",
    "\n",
    "    * $f$는 특성의 수, $c$는 클래스의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 비용함수**\n",
    "* 소프트맥스 회귀에서는 비용함수로 크로스 엔트로피 함수를 사용함. \n",
    "\n",
    "### **01. 크로스 엔트로피 함수**\n",
    "* 아래에서 $y$는 실제값을 나타내며, $k$는 클래스의 개수로 정의함. \n",
    "* $y_j$는 실제값 원-핫 벡터의 $j$번째 인덱스를 의미 \n",
    "* $p_j$는 샘플 데이터라 $j$번째 클래스일 확률은 나타냄. \n",
    "$$cost(W)=- \\sum_{j=1}^k y_jlog(p_j) $$\n",
    "* 만약 $p_1=1$인 경우 $ \\widehat{y}$가 {y}를 정확하게 예측한 경우가 됨. \n",
    "* 식으로 대입해보면 $-1*log(1)=0$이 되기 때문에, 결과적으로 $\\widehat{y}$가 $y$를 정확하게 예측한 경우 크로스 엔트로피 함수는 0이 됨. \n",
    "* 이를 $n$개의 전체 데이터에 대한 **퍙균**을 구한다고 하면 최종 비용함수는 다음과 같음. \n",
    "$$cost(W)=-\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^k y_j^{(i)}log(p_j^{i})$$\n",
    "\n",
    "### **02. 이진분류에서의 크로스 엔트로피 함수**\n",
    "* 본질적으로 로지스틱 회귀함수에서 배운 식과 동일한 식임. \n",
    "$$ cost(W) =  -(ylogH(X)) + (1-y)log(1-H(x)) $$\n",
    "* 위의 식에서 $y$를 $y_1$, $(1-y)$를 $y_2$로 치환하고, $H(x)$를 $p_1$, $-H(x)$를 $p_2$로 치환하면 아래의 식을 구할 수 있음. \n",
    "$$-(y_1log(p_1) + y_2log(p_2)) = -\\sum_{i=1}^2 y_ilog(p_i) $$\n",
    "* 결과적으로 소프트맥스 회귀식과 동일함. \n",
    "\n",
    "* **정리하자면**\n",
    "$$cost(W)=-\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^k y_j^{i}log(p_j^{i}) = -\\frac{1}{n}\\sum_{i=1}^n [y^{i}log(p^i) + (1-y^i)log(1-p^i)]$$\n",
    "소프트맥스 회귀와 로지스틱회귀의 비용함수는 동일함. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
