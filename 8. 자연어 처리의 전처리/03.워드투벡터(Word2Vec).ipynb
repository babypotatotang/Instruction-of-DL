{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMSX1OYqV1cASLkxUDAwIeA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babypotatotang/Introduction-to-DeepLearning/blob/main/8.%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC%EC%9D%98%20%EC%A0%84%EC%B2%98%EB%A6%AC/03.%EC%9B%8C%EB%93%9C%ED%88%AC%EB%B2%A1%ED%84%B0(Word2Vec).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어의 의미를 벡터화할 수 있는 가장 대표적인 방법은 `워드투벡터 (Word2Vec)`\n",
        "Word2Vec을 통해서 다음과 같은 작업을 수행할 수 있음. \n",
        "\n",
        "- 고양이 + 애교 = 강아지\n",
        "- 한국 - 서울 + 도쿄 = 일본 \n",
        "- 박찬호 - 야구 + 축구 = 호날두 "
      ],
      "metadata": {
        "id": "HQSYXVJYFf24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. 희소 표현(Sparse Representation)**\n",
        "* 원-핫 인코딩을 통해서 나온 벡터들은 sparse representation, 희소 표현에 해당함. 이러한 방법은 단어간 유사성을 표현할 수 없다는 단점이 있어 그 대안으로 단어의 '의미'를 다차원 공간에 벡터화하는 **분산 표현(distributed representation)** 방법이 등장하였음. \n",
        "* 저차원으 정보를 가지고 있어 해당 벡터를 **dense vector**라고도 함. \n",
        "\n",
        "# **2. 분산 표현(Distributed Representation)**\n",
        "* 분포 가설(distributed hypothesis)라는 가정하에 만들어진 표현. 이 가정은 **비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다**라는 가정임. \n",
        "    * 강아지라는 단어는 귀엽다, 예쁘다, 애교 등의 단어가 주로 등장하는 분포 가설에 따라 저런 내용을 가진 텍스트를 벡터화한다면 저 단어들은 의미적으로 가까운 단어가 됨. \n",
        "* 분산 표현은 저차원에서 **단어의 의미를 여러 차원에 분산**하여 표현하며 이런 표현 방법을 사용하면 **단어 간 유사도**를 계산할 수 있음. \n",
        "\n",
        "# **3.CBOW(Continuous Bag of Words)**\n",
        "### CBOW와 Skip-Gram 두 가지 방식이 있음.\n",
        "* CBOW는 주변에 있는 단어를 가지고, 중간에 있는 단어를 예측함. 반대로 Skip-Gram은 중간에 있는 단어로 주변 단어를 예측하는 방식임. \n",
        "\n",
        "## **CBOW**\n",
        "* 예문: The fat cat sat on the mat \n",
        "* 위 예문에서 CBOW가 해야하는 일은 `The fat cat ____ on the mat`에서 `sat`을 예측하는 것\n",
        "\n",
        "\n",
        "![](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)\n",
        "\n",
        "* 중심 단어를 예측하기 위해 앞, 뒤로 몇개의 단어를 볼지 결정함, 이때 이 수는 윈도우, window라고 함. \n",
        "* 예를 들어 윈도우 크기가 2이고, 예측하고자 하는 중심 단어가 `sat`이라면 앞뒤의`fat`, `cat`, `on`, `the`가 참고할 단어가 됨. \n",
        "\n",
        "\n",
        "![](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n",
        "\n",
        "* CBOW의 인공신경망을 도식화한 모습임. 입력층(input layer)의 입력으로서 앞, 뒤로 사용자가 정한 윈도우 크기 범위 안에 있는 주변 단어들의 **원-핫 벡터**가 들어가게 됨. \n",
        "* Word2Vec은 딥러닝 모델이 아니라, 입력층과 출력층 사이에 활성화 함수가 존재하지 않은 하나의 은닉층이 있는 **얕은 신경망**이 있음. 또 활성화 함수가 존재하지 않아 룩업 테이블이라는 연산을 담당하는 층으로 **투사층(projection layer)**가 존재함. \n",
        "\n",
        "![](https://wikidocs.net/images/page/22660/word2vec_renew_2.PNG)\n",
        "\n",
        "* 투사층의 크기는 **M**으로, CBOW를 수행하고 난 output vector의 크기 또한 M 차원을 가짐. \n",
        "* 입력층과 투사층 사이의 가중치 W는 V x M 행렬이며 투사층에서 출력층 사이의 가중치 W'는 M x v 행렬임. V는 단어 집합의 크기를 의미함. 이 두 행렬은 동일한 행렬을 전치한 것이 아니라, 서로 다른 행렬임. 인공 신경망의 훈련 전, 각 행렬은 아주 작은 랜덤값을 갖게 되고, 정확도를 얻으며 갱신하는 구조를 가짐. \n",
        "\n",
        "![](https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG)\n",
        "* 각 주변 단어의 원-핫 벡터를 $x$로 표기함. 입력 벡터는 원-핫 벡터임. i번째 인덱스에 1이라는 값을 가지고 나머지는 0이기 떄문에 W행렬의 i번째 행을 그대로 읽어오는(lookup) 것과 동일함. 그래서 이 작업을 **lookup table**이라고 부름. \n",
        "\n",
        "![](https://wikidocs.net/images/page/22660/word2vec_renew_4.PNG)\n",
        "* 마지막으로 4개의 입력 벡터의 평균을 구해 중간 단어를 예측함. \n",
        "\n",
        "![](https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG)\n",
        "* 이렇게 구해진 평균벡터는 두번째 가중치 행렬 W'와 곱해지며 그 결과로 원-핫 벡터들과 차원이 V인 동일한 벡터가 나옴. 이 이후에 `softmax`함수를 취해 score vector를 얻음. \n",
        "    * 스코어 벡터의 j번째 인덱스는 j번째 단어가 중심단어일 확률을나타냄. 그리고 이 스코어 벡터는 우리가 실제 중심 단어의 원-핫 벡터와 값이 가까워 져야 하며, 이를 줄이는 loss-function은 다음과 같음. \n",
        "\n",
        "![](https://wikidocs.net/images/page/22660/crossentrophy.PNG)\n",
        "\n",
        "# **4. Skip gram**\n",
        "* 중심단어에서 주변 단어를 예측함. \n",
        "\n",
        "![](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)\n",
        "\n",
        "* 앞서 언급한 동일한 예문에 대해 인공신경망을 도식화함. 중심 단어에 대해서 주변 단어를 예측하기 때문에 투사층에서 벡터들의 평균을 구하는 과정이 없음. 전반적으로 Skip-gram이 CBOW보다 대중적으로 사용됨. \n",
        "\n",
        "# **5. 네거티브 샘플링(Negative Sampling)**\n",
        "* 일반적으로 Word2Vec을 사용한다고 하면 SGNS(Skip-Gram with Negative Sampling)을 사용함. \n",
        "* Word2Vec의 경우 속도가 너무 오래걸림.모든 단어에 대한 임베딩을 조정하기 때문에 시간이 너무 오래 걸림. \n",
        "* 전체 단어 집합이 아닌, 일부 단어 집합에 대해서만 고려하고자, 주변 단어와 랜덤한 (전혀 상관없는) 단어들을 가져와 훨씬 작은 단어 집합을 만들어 줌. \n",
        "* 즉, 주변 단어는 positive, 랜덤 단어는 negative로 설정하여 이진 분류 문제를 수행함. "
      ],
      "metadata": {
        "id": "S5q5nDVfGNUw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "je7H8kk9GMpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}