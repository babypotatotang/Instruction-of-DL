{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPrLeJlp/fQdH9XJUkuzcTd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babypotatotang/Introduction-to-DeepLearning/blob/main/8.%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC%EC%9D%98%20%EC%A0%84%EC%B2%98%EB%A6%AC/02.%EC%9B%8C%EB%93%9C%20%EC%9E%84%EB%B2%A0%EB%94%A9(Word%20Embedding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "워드 임베딩은(Word Embedding)은 단어를 벡터로 표현하는 것을 말함. \n",
        "워드 임베딩은 단어를 **밀집 표현**으로 변환하는 방법을 말하며, 이번 챕터에서는 **희소 표현, 밀집 표현, 워드임베딩**에 대한 개념을 이해함. \n",
        "\n",
        "## **1. 희소 표현(Sparse Representation)** \n",
        "--- \n",
        "*  원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스 값만 1이고, 나머지는 전부 0으로 표현되는 벡터 표현 방법\n",
        "* 이렇게 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 **희소 표현(sparse representation)**이라고 함. (원-핫 벡터는 희소 벡터) \n",
        "* 이런 원-핫 벡터의 경우 공간적 낭비가 심하며 **단어 벡터 간 유사도를 표현할 수 없음.** "
      ],
      "metadata": {
        "id": "rnJaidNAg90g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGJzXcq8g1v-"
      },
      "outputs": [],
      "source": [
        "import torch "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원-핫 벡터 생성\n",
        "dog = torch.FloatTensor([1, 0, 0, 0, 0])\n",
        "cat = torch.FloatTensor([0, 1, 0, 0, 0])\n",
        "computer = torch.FloatTensor([0, 0, 1, 0, 0])\n",
        "netbook = torch.FloatTensor([0, 0, 0, 1, 0])\n",
        "book = torch.FloatTensor([0, 0, 0, 0, 1])"
      ],
      "metadata": {
        "id": "UHTws1Uqh_Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cosine_similarity(dog, cat, dim=0))\n",
        "print(torch.cosine_similarity(cat, computer, dim=0))\n",
        "print(torch.cosine_similarity(computer, netbook, dim=0))\n",
        "print(torch.cosine_similarity(netbook, book, dim=0))"
      ],
      "metadata": {
        "id": "Q2Z0RXMAiBZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. 밀집 표현(Dense Representation)**\n",
        "--- \n",
        "* 밀집표현은 벡터의 차원을 단어 집합의 크기로 상정하지 않음. \n",
        "* 사용자가 설정한 값으로 벡터 표현의 차원을 맞추며, 이 과정에서 0과 1만 가진 값이 아니라 실수 값을 가지게 되며, 벡터의 차원이 조밀해졌다고 하여 **밀집 벡터(dense vector)**라고 표현함. "
      ],
      "metadata": {
        "id": "-mVhAppViDlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. 워드 임베딩(Word Embedding)**\n",
        "--- \n",
        "* 단어를 밀집벡터의 형태로 표현하는 방법을 **워드 임베딩(word embedding)**이라고 함. 이 밀집벡터를 임베딩 과정을 통해 나온 결과는 임베딩 벡터(embedding vector)라고 함. \n",
        "\n",
        "* 워드 임베딩 방법론으로 LSA, Word2Vec, FastText, Glove 등이 있음 \n",
        "\n",
        "|-|원-핫 벡터|임베딩 벡터|\n",
        "|------|------|------|\n",
        "|차원|고차원(단어 집합의 크기)| 저차원|\n",
        "|다른 표현|희소 벡터|밀집 벡터의 일종|\n",
        "|표현 방법|수동|훈련 데이터로부터 학습함|\n",
        "|값의 타입|1과 0|실수|"
      ],
      "metadata": {
        "id": "45QXM8xlikZk"
      }
    }
  ]
}