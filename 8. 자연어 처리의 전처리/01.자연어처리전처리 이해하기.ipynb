{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOnnuJyxqFppzQTnAbGewl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babypotatotang/Introduction-to-DeepLearning/blob/main/8.%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC%EC%9D%98%20%EC%A0%84%EC%B2%98%EB%A6%AC/01.%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC%EC%A0%84%EC%B2%98%EB%A6%AC%20%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **01. 자연어 처리 전처리 이해하기**\n",
        "--- \n",
        "* 일반적으로 **토큰화, 단어 집합 생성, 정수 인코딩, 패딩, 벡터화의 과정이 있음. \n",
        "\n",
        "## 1. 토큰화 (Tokenization) \n",
        "* 주어진 텍스트를 단어 또는 문자 단위로 자르는 것 \n",
        "* 영어의 경우 `spaCy`와 `NLTK`가 있음, 기본적으로 파이썬 `spli`으로도 토큰화할 수 있음. \n",
        "\n",
        "### 1. spaCy 사용하기 "
      ],
      "metadata": {
        "id": "7IddVjqWqrgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_text = \"A Dog Run back corner near spare bedrooms\""
      ],
      "metadata": {
        "id": "DwGS2fsw0sCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "MQKf31_u01QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(en_text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(en_text)]"
      ],
      "metadata": {
        "id": "miWldjd000EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize(en_text))"
      ],
      "metadata": {
        "id": "6lP9hQcxVPuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. NLTK 사용하기"
      ],
      "metadata": {
        "id": "QuCWv9aIVT_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "WqbpDOK_VWDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "DDhlbWfFVYbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize\n",
        "      (en_text))"
      ],
      "metadata": {
        "id": "_mmm5OtAVb3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 띄어쓰기로 토큰화"
      ],
      "metadata": {
        "id": "6k-vwgBSV5sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(en_text.split())"
      ],
      "metadata": {
        "id": "js_9LnZrVnr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 한국어 띄어쓰기 토큰화 "
      ],
      "metadata": {
        "id": "y0aXWXYdV_zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\""
      ],
      "metadata": {
        "id": "QzZMQUw3WDhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(kor_text.split())"
      ],
      "metadata": {
        "id": "5EZnOuubWEzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `사과`라는 단어가 4번 등장하는데 각각 다른 조사와 붙어서 등장함. 그럼 기계는 전부 다른 단어로 인식함. "
      ],
      "metadata": {
        "id": "z7iylZSWWIQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 형태소 토큰화 \n",
        "* 한국어는 보통 형태소 분석기로 토큰화를 진행함. 그 중 `mecab`을 사용해서 시도하겠음."
      ],
      "metadata": {
        "id": "HU09c3CDWPCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "metadata": {
        "id": "5Fx0cxn6WQ0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "tokenizer = Mecab()\n",
        "print(tokenizer.morphs(kor_text))"
      ],
      "metadata": {
        "id": "pGd_i69VWeoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 위와 같이 조사가 모두 분리된 것을 볼 수 있음. \n",
        "### 6. 문자 토큰화"
      ],
      "metadata": {
        "id": "FObWycKvYfuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(en_text))"
      ],
      "metadata": {
        "id": "kqXeYKFpYenp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 단어 집합 생성 \n",
        "--- \n",
        "단어 집합(vocabulary)란 **중복을 제거한** 텍스트의 총 단어의 집합을 의미함. "
      ],
      "metadata": {
        "id": "FJt-CaL1YrgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "from konlpy.tag import Mecab\n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "AmcT6GGpYqsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
        "data = pd.read_table('ratings.txt') # 데이터프레임에 저장\n",
        "data[:10]"
      ],
      "metadata": {
        "id": "ele-BDazZBs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"전체 샘플의 수: {len(data)}\")"
      ],
      "metadata": {
        "id": "uJvW1wI3ZCNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = data[:100] # 임의로 100개 저장 "
      ],
      "metadata": {
        "id": "hOlbHOQaZmhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-하-|가-힣 ]\", \"\") # 한글과 공백을 제외하고 모두 제거\n",
        "sample_data[:10]"
      ],
      "metadata": {
        "id": "K4e0Viu5ZrW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 정의\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
      ],
      "metadata": {
        "id": "pQRuRmzdZ9hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Mecab()\n",
        "\n",
        "tokenized = []\n",
        "for sentence in sample_data['document']:\n",
        "    tmp = tokenizer.morphs(sentence) # 토큰화 \n",
        "    tmp = [word for word in tmp if not word in stopwords] # 불용어 제거\n",
        "    tokenized.append(tmp)"
      ],
      "metadata": {
        "id": "0DdAWi7XaCBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized[:10])"
      ],
      "metadata": {
        "id": "SlL1GPOLaZkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = FreqDist(np.hstack(tokenized))\n",
        "print(f'단어 집합의 크기 : {len(vocab)}')"
      ],
      "metadata": {
        "id": "V1i5eRu3acMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "CW6RfApCbmdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[\"ㅋㅋ\"]"
      ],
      "metadata": {
        "id": "Mw67axQfbpUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 500\n",
        "# 상위 vocab_size 개의 단어만 보존 \n",
        "vocab = vocab.most_common(vocab_size)\n",
        "print(f'단어 집합의 크기 {len(vocab)}')"
      ],
      "metadata": {
        "id": "t_SxbOtNbtJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 각 단어에 고유한 정수 부여\n",
        "--- \n",
        "`enumerate()`는 순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받아 인덱스를 순차적으로 리턴함 "
      ],
      "metadata": {
        "id": "nIiFE5-jb6oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 집합의 단어들에 고유한 index 부여 \n",
        "word_to_index = {word[0]: index + 2 for index, word in enumerate(vocab)}\n",
        "word_to_index['pad'] = 1\n",
        "word_to_index['unk'] = 0"
      ],
      "metadata": {
        "id": "MKWwRZhOcW3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized된 문장에서 각 word에 해당하는 index 부여 \n",
        "encoded = []\n",
        "for line in tokenized: # 입력 데이터에서 1줄씩 문장을 읽음\n",
        "    tmp = []\n",
        "    for w in line: # 각 줄에서 1개씩 \n",
        "        try:\n",
        "            tmp.append(word_to_index[w])\n",
        "        except:\n",
        "            tmp.append(word_to_index['unk']) # 단어 집합에 없는 단어의 경우 unk로 대체 \n",
        "\n",
        "        encoded.append(tmp)"
      ],
      "metadata": {
        "id": "LBr_-w2Ocj6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded[:10])"
      ],
      "metadata": {
        "id": "jtNJMB6Wc3ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 길이가 다른 문장들을 모두 동일한 길이로 바꿔주는 패딩 (padding)\n",
        "--- \n",
        "* 각 문장들을 모두 동일한 길이로 바꿔주는 패딩 작업 진행\n",
        "* 패딩작업이란 정해준 길이로 모든 샘플들의 길이를 맞춰주되, 정해준 길이보다 짧은 샘플들에는 `pad` 토큰을 추가하여 길이를 맞춰줌 "
      ],
      "metadata": {
        "id": "RFXIYKi2daTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(l) for l in encoded)\n",
        "min_len = min(len(l) for l in encoded)\n",
        "avg_len = (sum(map(len, encoded))/len(encoded))"
      ],
      "metadata": {
        "id": "Jo47ttF7d0v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('리뷰의 최대 길이 : ', max_len)\n",
        "print('리뷰의 최소 길이 : ', min_len)\n",
        "print('리뷰의 평균 길이 : ', avg_len)"
      ],
      "metadata": {
        "id": "c1QQqCFdeFMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist([len(s) for s in encoded], bins = 50)\n",
        "plt.xlabel('length of sample')\n",
        "plt.ylabel('number of sample')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FRKftKVUeqDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in encoded:\n",
        "    if len(line) < max_len:\n",
        "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 pad 토큰으로 채움 "
      ],
      "metadata": {
        "id": "JJIFV128eqOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded[:3])"
      ],
      "metadata": {
        "id": "-QjIYA2QfAqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 단어들을 고유한 정수로 맵핑하였음. 각 정수를 고유한 **단어 벡터**로 바꾸는 작업이 필요하며 이때는 원-핫 인코딩, 워드 임베딩 등의 방식이 있음.  "
      ],
      "metadata": {
        "id": "QndW_ssFfLH2"
      }
    }
  ]
}