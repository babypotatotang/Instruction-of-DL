{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 넘파이로 텐서 만들기\n",
    "## 벡터와 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy로 텐서 만들기 : (1) 숫자로 이루어진 리스트 선언 (2) np.array()로 감싸주기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 1D with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "t=np.array([0.,1.,2.,3.,4.,5.,6.]) \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of t:  1\n",
      "Shape of t:  (7,)\n"
     ]
    }
   ],
   "source": [
    "print('Rank of t: ',t.ndim) # 차원\n",
    "print('Shape of t: ',t.shape) # 크기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ndim: 몇 차원인지 출력, 1차원은 벡터, 2차원은 행렬, 3차원은 3차원 텐서\n",
    "* shape: 크기 (7,)=(1,7)=(1 x 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[0] t[1] t[-1] =  0.0 1.0 6.0\n"
     ]
    }
   ],
   "source": [
    "print('t[0] t[1] t[-1] = ', t[0],t[1],t[-1]) #인덱스를 통한 원소 접근"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 인덱스를 통해 원소에 접근할 수 있음. \n",
    "* (slicing) 범위를 지정하여 원소를 불러옴. \n",
    "    * [시작 번호: 끝 번호]: 시작번호<= i < 끝번호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t[2:5] t[4:-1] =  [2. 3. 4.] [4. 5.]\n",
      "t[:2] t[3:] =  [0. 1.] [3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "print('t[2:5] t[4:-1] = ', t[2:5], t[4:-1]) # [시작 번호: 끝 번호]로 범위 지정을 통해 접근할 수 있음. \n",
    "print('t[:2] t[3:] = ', t[:2],t[3:]) # 시작 번호를 생략한 경우와 끝 번호를 생략한 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 2D with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "t=np.array([[1.,2.,3.],[4.,5.,6.],[7.,8.,9.],[10.,11.,12.]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of t:  2\n",
      "Shape of t:  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Rank of t: ', t.ndim)\n",
    "print('Shape of t: ',t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* .ndim: 몇 차원인지 출력. 1차원 텐서: 벡터, 2차원: 행렬, 3차원: 3차원 텐서 \n",
    "* .shape: 크기를 출력. (4,3)=(4 x 3): 4행 3열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 파이토치 텐서 선언하기(Pytorch Tensor Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 1D with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t=torch.FloatTensor([0.,1.,2.,3.,4.,5.,6.])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "# 1차원 텐서, 7개 원소\n",
    "print(t.dim()) # rank (차원)\n",
    "print(t.shape) # shape\n",
    "print(t.size()) #shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.) tensor(6.)\n",
      "tensor([2., 3., 4.]) tensor([4., 5.])\n",
      "tensor([0., 1.]) tensor([3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(t[0],t[1],t[-1]) # 인덱스로 접근\n",
    "print(t[2:5],t[4:-1]) # 슬라이싱\n",
    "print(t[:2],t[3:]) # 슬라이싱 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 2D with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "t=torch.FloatTensor([[1.,2.,3.],\n",
    "                    [4.,5.,6.,],\n",
    "                    [7.,8.,9.,],\n",
    "                    [10.,11.,12.]])\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 2차원 (4,3) 크기의 텐서\n",
    "print(t.dim()) # rank, 차원\n",
    "print(t.size()) # shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  5.,  8., 11.])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(t[:,1]) #  첫번째 차원을 전체 선택, 두번째 차원은 첫번째 것만 가져옴\n",
    "print(t[:,1].size()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 첫번째 차원을 전체 선택\n",
    "* 두번째 차원의 1번 인덱스 값만을 가져옴 \n",
    "* 텐서에서 두번째 열에 있는 모든 값을 가져온 상황"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.],\n",
      "        [ 4.,  5.],\n",
      "        [ 7.,  8.],\n",
      "        [10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "print(t[:,:-1]) # 첫번째 차원을 전체 선택, 두번째 차원은 맨 마지막에서 첫번째를 제외하고 가져오는 경우 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 브로드캐시팅(Broadcasting)\n",
    "* 두 행렬 A,B가 있다고 가정,각 행렬 간의 사칙연산을 위해서는 다음의 조건이 있음. \n",
    "    * (덧셈과 뺄셈) 두 행렬 A,B의 크기가 같아야함. \n",
    "    * (곱셈) A의 마지막 차원과 B의 마지막 차원이 같아야함. \n",
    "* 딥러닝을 하게 될때 불가피하게 크기가 다른 행렬 또는 텐서에 대해서 사칙연산을 수행할 수 있음. \n",
    "* 이를 위해 파이토치에서는 **자동으로 크기를 맞춰서 연산을 수행하는 브로드캐스팅 기능**을 제공. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# 크기가 (1,2)로 같은 행렬간의 덧셈 연산\n",
    "m1=torch.FloatTensor([[3,3]])\n",
    "m2=torch.FloatTensor([[2,2]])\n",
    "print(m1+m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5.])\n"
     ]
    }
   ],
   "source": [
    "# Vector + Scalar\n",
    "m1=torch.FloatTensor([1,2])\n",
    "m2=torch.FloatTensor([3]) # [3] -> [3,3] (브로드캐스팅)\n",
    "print(m1+m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 x 1 Vector + 1 x 2 Vector\n",
    "m1=torch.FloatTensor([[1,2]])\n",
    "m2=torch.FloatTensor([[3],[4]])\n",
    "print(m1+m2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 브로드캐스팅 과정에서 두 텐서는 다음과 같이 변경됨. \n",
    "    * [[1,2],[1,2]]\n",
    "    * [[3,3],[4,4]]\n",
    "* 자동으로 텐서의 크기를 변화시켜주어 편리하지만, 원하는 결과가 나오지 않을 수 있으므로 주의해서 사용해야함. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 자주 사용되는 기능들\n",
    "### [1] 행렬 곱셈과 곱셈의 차이 (Matrix Multiplication vs Multiplication)\n",
    "* .matmul 행렬 곱셈\n",
    "* .mul 곱셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n"
     ]
    }
   ],
   "source": [
    "m1=torch.FloatTensor([[1,2],[3,4]])\n",
    "m2=torch.FloatTensor([[1],[2]])\n",
    "print('Shape of Matrix 1: ', m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ',m2.shape) # 2 x 1\n",
    "print(m1.matmul(m2)) # 2 x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* element-wise 곱셈이 존재함. \n",
    "* 동일한 크기의 행렬이 동일한 위치에 있는 원소끼리 곱하는 것을 말함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix 1:  torch.Size([2, 2])\n",
      "Shape of Matrix 2:  torch.Size([2, 1])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "m1=torch.FloatTensor([[1,2],[3,4]])\n",
    "m2=torch.FloatTensor([[1],[2]])\n",
    "print('Shape of Matrix 1: ',m1.shape) # 2 x 2\n",
    "print('Shape of Matrix 2: ', m2.shape) # 2 x 1\n",
    "print(m1 * m2) # 2 x 2\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 두가지 곱셈에서 m2는 다음의 행렬로 브로드캐시틍이 된 후에 곱셈이 수행됐음. \n",
    "    * [1]   \n",
    "      [2]    \n",
    "      ==> [[1,1],            \n",
    "                [2,2]]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] 평균(Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5000)\n"
     ]
    }
   ],
   "source": [
    "t=torch.FloatTensor([1,2])\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(2.5000)\n"
     ]
    }
   ],
   "source": [
    "t=torch.FloatTensor([[1,2],[3,4]])\n",
    "print(t)\n",
    "print(t.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dim=0이라는 것은 첫번째 차원을 의미함. (행)\n",
    "* 인자로 dim을 준다면 해당 차원을 제거한다는 뜻이 됨 -> 행열에서 '열' 만 남게 됨. \n",
    "* 실제 연산 과정\n",
    "    t.mean(dim=0)은 입력에서 첫번째 차원을 제거한다.   \n",
    "    [[1., 2.],  \n",
    "     [3., 4.]]   \n",
    "    1과 3의 평균을 구하고, 2와 4의 평균을 구한다.    \n",
    "    결과 ==> [2., 3.]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] 덧셈(Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "t=torch.FloatTensor([[1,2],[3,4]])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor([4., 6.])\n",
      "tensor([3., 7.])\n",
      "tensor([3., 7.])\n"
     ]
    }
   ],
   "source": [
    "print(t.sum()) # 단순히 원소 전체의 덧셈을 수행\n",
    "print(t.sum(dim=0)) # 행을 제거\n",
    "print(t.sum(dim=1)) # 열을 제거\n",
    "print(t.sum(dim=-1)) # 열을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4] 최대(Max)와 아그맥스(ArgMax)\n",
    "* Max: 원소의 최대값을 리턴\n",
    "* ArgMax: 최대값을 가진 인덱스를 리ㅣ턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(4.)\n",
      "torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "t=torch.FloatTensor([[1,2],[3,4]])\n",
    "print(t)\n",
    "print(t.max())\n",
    "print(t.max(dim=0)) # max와 argmax를 모두 return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* max에 dim 인자를 주면 argmax도 함께 리턴함. \n",
    "* 최대값(max)와 최대값의 인덱스값(argmax)를 리턴함. \n",
    "* max 또는 argmax만 리턴받고 시다면 리턴값에 인덱스를 부여\n",
    "    * 0법 인덱스: max\n",
    "    * 1번 인덱스: argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  tensor([3., 4.])\n",
      "Argmax:  tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print('Max: ', t.max(dim=0)[0])\n",
    "print('Argmax: ', t.max(dim=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim=1))\n",
    "print(t.max(dim=-1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33bb709aa52d0f1c012e153b74c2c585d1723b1d162db1a778a7381b9d9efdce"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
