{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64134f5e",
   "metadata": {},
   "source": [
    "# optimizer.zero_grad(), loss.backward(), optimizer.step()\n",
    "\n",
    "* `optimizer.zero_grad()` : 이전 step에서 각 layer 별로 계산된 gradient 값을 모두 0으로 초기화, 0으로 초기화하지 않으면 이전 step의 결과에 현재 gradient가 누적으로 합해져서 계산됨. \n",
    "* `loss.backward()`: 각 layer의 파라미터에 대하여 back-propagation을 통해 graident를 계산 \n",
    "* `optimizer.step()`: 각 layer의 파라미터와 같이 저장된 gradient값을 이용하여 파라미터를 업데이트함. (모델의 성능이 개선) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f102ea",
   "metadata": {},
   "source": [
    "# optimizer.step()을 통한 파라미터 업데이트와 loss.backward()와의 관계 \n",
    "\n",
    "* pytorch에서 학습시, weight를 업데이트하는 시점은 `optimizer.step()`이 실행되는 시점 \n",
    "* `optimizer.step()`을 사용하는 순서를 확인하면, **NN의 출력값과 라벨값을 loss 함수를 이용하여 계산**하고, 그 loss 함수의 `.backward()` 연산을 한 뒤에 `optimizer.step()`을 통해 **weight를 업데이트함.** \n",
    "\n",
    "## loss와 optimizer 선언 및 사용 예시 \n",
    "\n",
    "* `loss`의 선언 및 사용은 다음과 같음. \n",
    "\n",
    "\n",
    "```python \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "out = model(input) # model 예측\n",
    "\n",
    "loss = criterion(out, target) # (1) loss 계산\n",
    "loss.backward() # (2) gradient 계산 \n",
    "```\n",
    "* `optimizer`의 선언 및 사용은 다음과 같음. \n",
    "\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr) \n",
    "optimizer.step() # (3) weight 업데이트 \n",
    "```\n",
    "\n",
    "## 프레임워크 설명 \n",
    "\n",
    "* 프레임워크 순서상 (1) loss 계산, (2) loss.backward()로 gradient 계산, (3) optimizer.step()으로 weight 업데이트 \n",
    "* 이때 `loss`와 `opimizer`의 연결 포인트는 NN가 선언된 객체인 `model`의 각각이 가지는 **weight의 gradient 값**임.\n",
    "\n",
    "* 예를 들어 model의 convolution 레이어 중 하나의 이름이 conv1이라면, `model.conv1.weight.grad`에 loss에 따라 계산된 **gradient가 저장됨.**\n",
    "\n",
    "1. `loss.backward()`가 실행되면 , `~.weight.grad`에 gradient가 저장됨. \n",
    "2. `optimizer` 객체는 `model.parameters()`를 통해 생성되었기 때문에, `loss.backward()`를 통해 `~.weight.grad`에 저장된 각 layer의 gradient는 `optimizer`에서 바로 접근하여 사용할 수 있음. \n",
    "3. 따라서 `optimizer`와 `loss.backward()`는 같은 model 객체 (parameter)를 사용하고 `loss.backward()`의 출력값이 각 model 객체 layer의 grad 멤버 변수에 저장되는 방식으로 두 연산이 연결됨. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
